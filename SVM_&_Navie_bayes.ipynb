{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Information Gain is a metric used to measure how well a feature splits data into distinct classes. It is based on entropy, which quantifies impurity.\n",
        "\n",
        "Information Gain = Entropy(before split) – Weighted Entropy(after split)\n",
        "In Decision Trees, the feature with the highest Information Gain is selected as the splitting feature at each node because it results in the most significant reduction in impurity and helps build an efficient, accurate tree."
      ],
      "metadata": {
        "id": "XSSS3__q9WIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "| Metric            | Formula                 | Interpretation                            | When Useful                              |\n",
        "| ----------------- | ----------------------- | ----------------------------------------- | ---------------------------------------- |\n",
        "| **Gini Impurity** | (1 - \\sum p_i^2)        | Measures probability of misclassification | Faster, preferred in CART                |\n",
        "| **Entropy**       | (-\\sum p_i \\log_2(p_i)) | Measures disorder/uncertainty             | Used when probabilistic purity is needed |\n",
        "\n",
        "Key differences:\n",
        "\n",
        "Gini is computationally simpler and often preferred.\n",
        "\n",
        "Entropy tends to produce slightly more balanced trees.\n",
        "\n",
        "Both measure impurity; choice depends on performance needs."
      ],
      "metadata": {
        "id": "_P75ZHP79ZQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning (also called early stopping) prevents a decision tree from growing too large by stopping the splitting process early when further splits are unlikely to improve performance.\n",
        "\n",
        "Common stopping criteria:\n",
        "\n",
        "Minimum number of samples required to split a node\n",
        "\n",
        "Maximum depth of the tree\n",
        "\n",
        "Minimum leaf size\n",
        "\n",
        "Minimum impurity decrease\n",
        "\n",
        "It reduces overfitting and improves generalization."
      ],
      "metadata": {
        "id": "ywMS_g9g9m9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4: Python program — Decision Tree with Gini Impurity\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LllTiPQa9p4d",
        "outputId": "f033cb3e-da00-46a2-f730-c9ac820fcc52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "SVM is a supervised learning algorithm used for classification and regression. It works by finding the optimal hyperplane that maximally separates classes.\n",
        "\n",
        "Key idea: maximize the margin between support vectors (boundary points) and the separating hyperplane."
      ],
      "metadata": {
        "id": "bDyyOEXh903A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick allows SVMs to classify data that is not linearly separable by implicitly mapping it to a higher-dimensional space without computing the transformation directly.\n",
        "\n",
        "Popular kernels:\n",
        "\n",
        "Linear\n",
        "\n",
        "Polynomial\n",
        "\n",
        "RBF (Gaussian)\n",
        "\n",
        "This makes SVM powerful for complex decision boundaries."
      ],
      "metadata": {
        "id": "bvmEIsPz91-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Python program — Linear vs RBF SVM on Wine Dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "clf_linear = SVC(kernel='linear')\n",
        "clf_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, clf_linear.predict(X_test))\n",
        "\n",
        "# RBF SVM\n",
        "clf_rbf = SVC(kernel='rbf')\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, clf_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ9b5UnT957p",
        "outputId": "54cbc43d-eb4c-4679-860d-0725ca728b01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer:\n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem.\n",
        "It assumes that all features are independent given the class label.\n",
        "\n",
        "It is called “naïve” because this independence assumption rarely holds in real data, yet the classifier performs remarkably well in practice"
      ],
      "metadata": {
        "id": "K3QypjBG9_Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "\n",
        "Answer:\n",
        "| Type               | Data Type                                         | Use Case                                           |\n",
        "| ------------------ | ------------------------------------------------- | -------------------------------------------------- |\n",
        "| **Gaussian NB**    | Continuous features (follows normal distribution) | Medical data, sensor data                          |\n",
        "| **Multinomial NB** | Count data                                        | NLP, word counts                                   |\n",
        "| **Bernoulli NB**   | Binary features (0/1)                             | Document classification with word presence/absence |\n"
      ],
      "metadata": {
        "id": "-HCe9Ke0-BTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Python program — Gaussian NB on Breast Cancer Dataset\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Gaussian NB model\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKDwhdSM-Ilv",
        "outputId": "c69d85c8-117b-4d7c-e56a-463c57a438c3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}